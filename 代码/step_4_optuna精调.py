import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import optuna
from lightgbm import LGBMClassifier, early_stopping
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score
from sklearn.preprocessing import LabelEncoder
from datetime import datetime
import json
import os

# âœ… è·¯å¾„è®¾ç½®
file_path = r"E:\software\Jetbrains\Python Project\25_1__ML_learn\é¡¹ç›®ç»ƒä¹ _25_4_11_Health Insurance Cross Sell Prediction ğŸ  ğŸ¥\æ•°æ®æº\archive\train_å¤„ç†åæ•°æ®.csv"
output_base_path = r"E:\software\Jetbrains\Python Project\25_1__ML_learn\é¡¹ç›®ç»ƒä¹ _25_4_11_Health Insurance Cross Sell Prediction ğŸ  ğŸ¥\ç»“æœä¸€è§ˆ\step_4_optumaç²¾è°ƒ"
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = os.path.join(output_base_path, f"optuna_results_{timestamp}")
os.makedirs(output_path, exist_ok=True)

# âœ… æ•°æ®è¯»å–ä¸ç¼–ç 
df = pd.read_csv(file_path)
y = df["Response"]
X = df.drop(columns=["Response", "id"], errors="ignore")
for col in X.select_dtypes(include='object').columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# âœ… ä½¿ç”¨ 50% æ•°æ®æ ·æœ¬
X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.5, stratify=y, random_state=42)

# âœ… äº¤å‰éªŒè¯å™¨
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# âœ… optunaç›®æ ‡å‡½æ•°ï¼ˆæ›´æ–°åçš„å‚æ•°åŒºé—´ï¼‰
def objective(trial):
    params = {
        "boosting_type": "gbdt",
        "learning_rate": 0.05,
        "n_estimators": 1000,
        "num_leaves": trial.suggest_int("num_leaves", 3, 11),
        "max_depth": trial.suggest_int("max_depth", 8, 32),
        "subsample": trial.suggest_float("subsample", 0.02, 0.2),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.12, 0.24),
        "scale_pos_weight": trial.suggest_float("scale_pos_weight", 2.6, 3.6),
        "min_child_samples": trial.suggest_int("min_child_samples", 30, 80),
        "min_child_weight": trial.suggest_float("min_child_weight", 0.0004, 0.024),
        "reg_alpha": trial.suggest_float("reg_alpha", 0.0016, 0.0028),
        "reg_lambda": trial.suggest_float("reg_lambda", 0.001, 3.0),

        "random_state": 42,
        "n_jobs": 3,
        "min_split_gain": 0.001,
    }

    # âœ… thresholdï¼šå·¦ä¾§å»¶æ‹“è‡³ 0.25ï¼Œå³è¾¹ä¿æŒä¸å˜
    threshold = trial.suggest_float("threshold", 0.1, 0.2)



    fold_f1 = []
    fold_auc = []
    fold_precision = []
    fold_recall = []
    best_iterations = []

    for train_idx, valid_idx in cv.split(X_sample, y_sample):
        X_train, X_valid = X_sample.iloc[train_idx], X_sample.iloc[valid_idx]
        y_train, y_valid = y_sample.iloc[train_idx], y_sample.iloc[valid_idx]

        model = LGBMClassifier(**params)
        model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            callbacks=[early_stopping(stopping_rounds=30, verbose=False)]
        )

        y_prob = model.predict_proba(X_valid)[:, 1]
        y_pred = (y_prob > threshold).astype(int)

        fold_f1.append(f1_score(y_valid, y_pred))
        fold_auc.append(roc_auc_score(y_valid, y_prob))
        fold_precision.append(precision_score(y_valid, y_pred))
        fold_recall.append(recall_score(y_valid, y_pred))
        best_iterations.append(model.best_iteration_)

    trial.set_user_attr("f1", np.mean(fold_f1))
    trial.set_user_attr("auc", np.mean(fold_auc))
    trial.set_user_attr("precision", np.mean(fold_precision))
    trial.set_user_attr("recall", np.mean(fold_recall))
    trial.set_user_attr("avg_best_iteration", np.mean(best_iterations))

    return np.mean(fold_recall)

# âœ… å¼€å§‹optunaè°ƒå‚
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=300, show_progress_bar=True)

# âœ… ä¿å­˜æœ€ä½³è¶…å‚æ•°
best_params = study.best_trial.params
with open(os.path.join(output_path, "best_params_optuna.json"), "w", encoding="utf-8") as f:
    json.dump(best_params, f, indent=2, ensure_ascii=False)

# âœ… ä¿å­˜å‰10ä¸ª trial æŒ‡æ ‡
top_trials = sorted(study.trials, key=lambda x: x.user_attrs.get("f1", -1), reverse=True)[:10]

top10_data = []
for t in top_trials:
    record = {**t.params}
    record.update({
        "f1_score": t.user_attrs["f1"],
        "auc": t.user_attrs["auc"],
        "precision": t.user_attrs["precision"],
        "recall": t.user_attrs["recall"],
        "avg_best_iteration": t.user_attrs["avg_best_iteration"],
    })
    top10_data.append(record)

df_top10 = pd.DataFrame(top10_data)
df_top10.to_csv(os.path.join(output_path, "top10_trials_metrics.csv"), index=False, encoding="utf-8-sig")

# âœ… ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆåŸºäºæœ€ä½³æ¨¡å‹ï¼‰
model_best = LGBMClassifier(**best_params, learning_rate=0.05, n_estimators=1000)
model_best.fit(X_sample, y_sample)
importances = model_best.feature_importances_
feat_names = X_sample.columns

plt.figure(figsize=(10, 6))
plt.barh(feat_names, importances)
plt.title("Feature Importances")
plt.xlabel("Importance")
plt.tight_layout()
plt.savefig(os.path.join(output_path, "feature_importance.png"))
plt.close()

# âœ… ç»˜åˆ¶è¶…å‚æ•°é‡è¦æ€§å›¾
optuna.visualization.matplotlib.plot_param_importances(study)
plt.tight_layout()
plt.savefig(os.path.join(output_path, "hyperparameter_importance.png"))
plt.close()

# âœ… ç»˜åˆ¶æ¯ä¸ªå‚æ•° vs F1-score / Recall çš„æ•£ç‚¹å›¾
params_to_plot = ["threshold", "scale_pos_weight", "num_leaves", "max_depth",
                  "subsample", "colsample_bytree", "min_child_samples",
                  "min_child_weight", "reg_alpha", "reg_lambda"]

df_trials = pd.DataFrame([{**t.params, **t.user_attrs} for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])

for param in params_to_plot:
    if param not in df_trials.columns:
        continue

    plt.figure(figsize=(8, 5))
    plt.scatter(df_trials[param], df_trials["f1"], label="F1-score", alpha=0.7)
    plt.scatter(df_trials[param], df_trials["recall"], label="Recall", alpha=0.7)
    plt.title(f"{param} vs F1-score & Recall")
    plt.xlabel(param)
    plt.ylabel("Score")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.savefig(os.path.join(output_path, f"{param}_vs_f1_recall.png"))
    plt.close()

print(f"âœ… Optunaè°ƒå‚å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_path}")

# âœ… ç”Ÿæˆ Optuna å‚æ•°äº¤äº’å›¾ï¼ˆContourã€Sliceã€Parallelï¼‰
import optuna.visualization as vis

# åˆ›å»ºç”¨äºå­˜å‚¨äº¤äº’å›¾çš„å­ç›®å½•
timestamp_interaction = datetime.now().strftime("%Y%m%d_%H%M%S")
interaction_path = os.path.join(output_base_path, f"optuna_interactions_{timestamp_interaction}")
os.makedirs(interaction_path, exist_ok=True)

# âœ… ç”Ÿæˆè½®å»“å›¾ï¼ˆå±•ç¤ºå‚æ•°äº¤äº’ï¼‰
param_pairs = [
    ("threshold", "scale_pos_weight"),
    ("subsample", "colsample_bytree"),
    ("min_child_samples", "min_child_weight"),
    ("max_depth", "num_leaves"),
    ("reg_alpha", "reg_lambda")
]

for x, y in param_pairs:
    fig = vis.plot_contour(study, params=[x, y])
    fig.write_image(os.path.join(interaction_path, f"contour_{x}_vs_{y}.png"))

# âœ… å¹³è¡Œåæ ‡å›¾
fig = vis.plot_parallel_coordinate(study)
fig.write_image(os.path.join(interaction_path, "parallel_coordinate.png"))

# âœ… åˆ‡ç‰‡å›¾
fig = vis.plot_slice(study)
fig.write_image(os.path.join(interaction_path, "slice_plot.png"))

# âœ… å‚æ•°é‡è¦æ€§å›¾ï¼ˆå¤‡ä»½ï¼‰
fig = vis.plot_param_importances(study)
fig.write_image(os.path.join(interaction_path, "hyperparameter_importance.png"))

print(f"âœ… å‚æ•°äº¤äº’å›¾ç”Ÿæˆå®Œæˆï¼Œä¿å­˜è‡³: {interaction_path}")

# ---------------------------
# 5. Best-value å†å²å›¾
# ---------------------------
import optuna.visualization as vis

# 5.1 Recall æœ€ä¼˜å€¼å†å²ï¼ˆPlotly ç‰ˆï¼‰
fig_rec = vis.plot_optimization_history(study)
fig_rec.update_layout(
    title="Optuna Optimization History (Recall)",
    xaxis_title="Trial",
    yaxis_title="Recall",
)
fig_rec.write_image(os.path.join(output_path, "opt_history_recall_plotly.png"))

# 5.2 F1-score æœ€ä¼˜å€¼å†å²ï¼ˆMatplotlib ç‰ˆæ‰‹å·¥ç”»ï¼‰
#    ä» user_attrs é‡Œå– F1
trials_completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
trial_ids  = [t.number for t in trials_completed]
f1_values  = [t.user_attrs["f1"] for t in trials_completed]

plt.figure(figsize=(8, 5))
plt.plot(trial_ids, f1_values, marker='o', linestyle='-', color='tab:blue', label="F1-score")
plt.title("Optuna Optimization History (F1-score)")
plt.xlabel("Trial Number")
plt.ylabel("F1-score")
plt.grid(True)
plt.tight_layout()
plt.savefig(os.path.join(output_path, "opt_history_f1_matplotlib.png"))
plt.close()

print("âœ… Recall/F1 æœ€ä¼˜å€¼å†å²å›¾ å·²ä¿å­˜è‡³ï¼š", output_path)





"""
æ—§ç‰ˆä»£ç   F1åˆ†æ•°ä¸ºæŒ‡æ ‡
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import optuna
from lightgbm import LGBMClassifier, early_stopping
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score
from sklearn.preprocessing import LabelEncoder
from datetime import datetime
import json
import os

# âœ… è·¯å¾„è®¾ç½®
file_path = r"E:\software\Jetbrains\Python Project\25_1__ML_learn\é¡¹ç›®ç»ƒä¹ _25_4_11_Health Insurance Cross Sell Prediction ğŸ  ğŸ¥\æ•°æ®æº\archive\train_å¤„ç†åæ•°æ®.csv"
output_base_path = r"E:\software\Jetbrains\Python Project\25_1__ML_learn\é¡¹ç›®ç»ƒä¹ _25_4_11_Health Insurance Cross Sell Prediction ğŸ  ğŸ¥\ç»“æœä¸€è§ˆ\step_4_optumaç²¾è°ƒ"
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = os.path.join(output_base_path, f"optuna_results_{timestamp}")
os.makedirs(output_path, exist_ok=True)

# âœ… æ•°æ®è¯»å–ä¸ç¼–ç 
df = pd.read_csv(file_path)
y = df["Response"]
X = df.drop(columns=["Response", "id"], errors="ignore")
for col in X.select_dtypes(include='object').columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# âœ… ä½¿ç”¨ 50% æ•°æ®æ ·æœ¬
X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.5, stratify=y, random_state=42)

# âœ… äº¤å‰éªŒè¯å™¨
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# âœ… optunaç›®æ ‡å‡½æ•°ï¼ˆæ›´æ–°åçš„å‚æ•°åŒºé—´ï¼‰
def objective(trial):
    params = {
        "boosting_type": "gbdt",
        "learning_rate": 0.05,
        "n_estimators": 1000,
        "num_leaves": trial.suggest_int("num_leaves", 5, 13),
        "max_depth": trial.suggest_int("max_depth", 5, 24),
        "subsample": trial.suggest_float("subsample", 0.1, 0.25),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.25, 0.4),
        "scale_pos_weight": trial.suggest_float("scale_pos_weight", 2.0, 2.5),
        "min_child_samples": trial.suggest_int("min_child_samples", 50, 100),
        "min_child_weight": trial.suggest_float("min_child_weight", 0.0004, 0.015),
        "reg_alpha": trial.suggest_float("reg_alpha", 0.0016, 0.003),
        "reg_lambda": trial.suggest_float("reg_lambda", 0.5, 6.0),

        "random_state": 42,
        "n_jobs": 3,
        "min_split_gain": 0.001,
    }

    # âœ… thresholdï¼šå·¦ä¾§å»¶æ‹“è‡³ 0.25ï¼Œå³è¾¹ä¿æŒä¸å˜
    threshold = trial.suggest_float("threshold", 0.26, 0.4)



    fold_f1 = []
    fold_auc = []
    fold_precision = []
    fold_recall = []
    best_iterations = []

    for train_idx, valid_idx in cv.split(X_sample, y_sample):
        X_train, X_valid = X_sample.iloc[train_idx], X_sample.iloc[valid_idx]
        y_train, y_valid = y_sample.iloc[train_idx], y_sample.iloc[valid_idx]

        model = LGBMClassifier(**params)
        model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            callbacks=[early_stopping(stopping_rounds=30, verbose=False)]
        )

        y_prob = model.predict_proba(X_valid)[:, 1]
        y_pred = (y_prob > threshold).astype(int)

        fold_f1.append(f1_score(y_valid, y_pred))
        fold_auc.append(roc_auc_score(y_valid, y_prob))
        fold_precision.append(precision_score(y_valid, y_pred))
        fold_recall.append(recall_score(y_valid, y_pred))
        best_iterations.append(model.best_iteration_)

    trial.set_user_attr("f1", np.mean(fold_f1))
    trial.set_user_attr("auc", np.mean(fold_auc))
    trial.set_user_attr("precision", np.mean(fold_precision))
    trial.set_user_attr("recall", np.mean(fold_recall))
    trial.set_user_attr("avg_best_iteration", np.mean(best_iterations))

    return np.mean(fold_f1)

# âœ… å¼€å§‹optunaè°ƒå‚
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=300, show_progress_bar=True)

# âœ… ä¿å­˜æœ€ä½³è¶…å‚æ•°
best_params = study.best_trial.params
with open(os.path.join(output_path, "best_params_optuna.json"), "w", encoding="utf-8") as f:
    json.dump(best_params, f, indent=2, ensure_ascii=False)

# âœ… ä¿å­˜å‰10ä¸ª trial æŒ‡æ ‡
top_trials = sorted(study.trials, key=lambda x: x.user_attrs.get("f1", -1), reverse=True)[:10]

top10_data = []
for t in top_trials:
    record = {**t.params}
    record.update({
        "f1_score": t.user_attrs["f1"],
        "auc": t.user_attrs["auc"],
        "precision": t.user_attrs["precision"],
        "recall": t.user_attrs["recall"],
        "avg_best_iteration": t.user_attrs["avg_best_iteration"],
    })
    top10_data.append(record)

df_top10 = pd.DataFrame(top10_data)
df_top10.to_csv(os.path.join(output_path, "top10_trials_metrics.csv"), index=False, encoding="utf-8-sig")

# âœ… ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆåŸºäºæœ€ä½³æ¨¡å‹ï¼‰
model_best = LGBMClassifier(**best_params, learning_rate=0.05, n_estimators=1000)
model_best.fit(X_sample, y_sample)
importances = model_best.feature_importances_
feat_names = X_sample.columns

plt.figure(figsize=(10, 6))
plt.barh(feat_names, importances)
plt.title("Feature Importances")
plt.xlabel("Importance")
plt.tight_layout()
plt.savefig(os.path.join(output_path, "feature_importance.png"))
plt.close()

# âœ… ç»˜åˆ¶è¶…å‚æ•°é‡è¦æ€§å›¾
optuna.visualization.matplotlib.plot_param_importances(study)
plt.tight_layout()
plt.savefig(os.path.join(output_path, "hyperparameter_importance.png"))
plt.close()

# âœ… ç»˜åˆ¶æ¯ä¸ªå‚æ•° vs F1-score / Recall çš„æ•£ç‚¹å›¾
params_to_plot = ["threshold", "scale_pos_weight", "num_leaves", "max_depth",
                  "subsample", "colsample_bytree", "min_child_samples",
                  "min_child_weight", "reg_alpha", "reg_lambda"]

df_trials = pd.DataFrame([{**t.params, **t.user_attrs} for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])

for param in params_to_plot:
    if param not in df_trials.columns:
        continue

    plt.figure(figsize=(8, 5))
    plt.scatter(df_trials[param], df_trials["f1"], label="F1-score", alpha=0.7)
    plt.scatter(df_trials[param], df_trials["recall"], label="Recall", alpha=0.7)
    plt.title(f"{param} vs F1-score & Recall")
    plt.xlabel(param)
    plt.ylabel("Score")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.savefig(os.path.join(output_path, f"{param}_vs_f1_recall.png"))
    plt.close()

print(f"âœ… Optunaè°ƒå‚å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_path}")

# âœ… ç”Ÿæˆ Optuna å‚æ•°äº¤äº’å›¾ï¼ˆContourã€Sliceã€Parallelï¼‰
import optuna.visualization as vis

# åˆ›å»ºç”¨äºå­˜å‚¨äº¤äº’å›¾çš„å­ç›®å½•
timestamp_interaction = datetime.now().strftime("%Y%m%d_%H%M%S")
interaction_path = os.path.join(output_base_path, f"optuna_interactions_{timestamp_interaction}")
os.makedirs(interaction_path, exist_ok=True)

# âœ… ç”Ÿæˆè½®å»“å›¾ï¼ˆå±•ç¤ºå‚æ•°äº¤äº’ï¼‰
param_pairs = [
    ("threshold", "scale_pos_weight"),
    ("subsample", "colsample_bytree"),
    ("min_child_samples", "min_child_weight"),
    ("max_depth", "num_leaves"),
    ("reg_alpha", "reg_lambda")
]

for x, y in param_pairs:
    fig = vis.plot_contour(study, params=[x, y])
    fig.write_image(os.path.join(interaction_path, f"contour_{x}_vs_{y}.png"))

# âœ… å¹³è¡Œåæ ‡å›¾
fig = vis.plot_parallel_coordinate(study)
fig.write_image(os.path.join(interaction_path, "parallel_coordinate.png"))

# âœ… åˆ‡ç‰‡å›¾
fig = vis.plot_slice(study)
fig.write_image(os.path.join(interaction_path, "slice_plot.png"))

# âœ… å‚æ•°é‡è¦æ€§å›¾ï¼ˆå¤‡ä»½ï¼‰
fig = vis.plot_param_importances(study)
fig.write_image(os.path.join(interaction_path, "hyperparameter_importance.png"))

print(f"âœ… å‚æ•°äº¤äº’å›¾ç”Ÿæˆå®Œæˆï¼Œä¿å­˜è‡³: {interaction_path}")

# âœ… ç»˜åˆ¶ Optimization History Plot å¹¶ä¿å­˜ï¼ˆF1-scoreï¼‰
import optuna.visualization as vis  # ç”¨ Plotly ç‰ˆ
# å…ˆç”Ÿæˆ Plotly Figure
fig1 = vis.plot_optimization_history(study)
# å†™æ–‡ä»¶åˆ°ç£ç›˜
fig1.write_image(os.path.join(output_path, "optimization_history_f1.png"))

# âœ… ç»˜åˆ¶ Optimization History Plot å¹¶ä¿å­˜ï¼ˆRecallï¼‰
# å¯¹äº recallï¼Œæˆ‘ä»¬è‡ªå·±ç»„ç»‡ä¸€ä¸‹æ•°æ®ï¼Œç„¶åç”¨ matplotlib ç”»
import matplotlib.pyplot as plt

recalls = [t.user_attrs["recall"] for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
trials = [t.number for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]

plt.figure(figsize=(8, 5))
plt.plot(trials, recalls, marker='o', linestyle='-', color='tab:red', label='Recall')
plt.title("Optuna Optimization History (Recall)")
plt.xlabel("Trial Number")
plt.ylabel("Recall")
plt.grid(True)
plt.tight_layout()
plt.savefig(os.path.join(output_path, "optimization_history_recall.png"))
plt.close()

print("âœ… æœ€ä½³å€¼å†å²å›¾ï¼ˆF1-score & Recallï¼‰å·²ä¿å­˜")



"""